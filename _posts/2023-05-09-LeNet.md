---
layout: post
title: 图像分类及经典CNN实现之（一）———— LeNet
gh-repo: 吕昊佳/beautiful-jekyll
comments: true
---

Lenet 是一系列网络的合称，包括 Lenet1 - Lenet5，由 Yann LeCun 等人在 1990 年《Handwritten Digit Recognition with a Back-Propagation Network》中提出，是卷积神经网络的 HelloWorld。
### LeNet5网络结构
Lenet是一个 7 层的神经网络，包含 3 个卷积层，2 个池化层，1 个全连接层。其中所有卷积层的所有卷积核都为 5x5，步长 strid=1，池化方法都为全局 pooling，激活函数为 Sigmoid，网络结构如下：

![Crepe](/assets/img/LeNet网络结构1.png)

![Crepe](/assets/img/LeNet网络结构2.jpg)

### LeNet代码实现
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__() # super().__init__()调用父类的init方法
        # 卷积层C1
        self.conv1 = nn.Conv2d(1, 6, kernel_size=(5, 5)) # 输入通道1, 输出通道6, 卷积核大小为5x5
        # 池化层S2
        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2) # 窗口大小2x2, 输入与输出通道均为6
        # 卷积层C3
        self.conv2 = nn.Conv2d(6, 16, kernel_size=(5, 5))
        # 池化层S4
        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2) # 窗口大小2x2, 输入与输出通道均为16
        
        self.fc0 = nn.Linear(16 * 4 * 4, 120) # 把第三个卷积当作全连接层
        # 全连接层F
        self.fc1 = nn.Linear(120, 84) # 神经元个数设置为84
        # 全连接层(输出层)
        self.fc2 = nn.Linear(84, 10) # 分为10类

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool1(F.relu(x))
        x = self.conv2(x)
        x = self.pool2(F.relu(x))
        x = torch.flatten(x, 1)
        x = self.fc0(F.relu(x))
        x = self.fc1(F.relu(x))
        x = self.fc2(F.relu(x))
        
        return x
```

### 参考
https://blog.csdn.net/DIPDWC/article/details/117249500
